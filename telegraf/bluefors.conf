[global_tags]

[agent]
  ## Default data collection interval for all inputs
  interval = "30s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 100000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  logfile = "C:\\Program Files\\InfluxData\\telegraf\\logs\\telegraf.log"

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  logfile_rotation_interval = "1d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  logfile_rotation_max_archives = 7

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  log_with_timezone = "local"

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = true

# Configuration for sending metrics to InfluxDB
[[outputs.influxdb]]
  urls = ["${TELEGRAF_VMURL}"]

  username = "${TELEGRAF_VMUSER}"
  password = "${TELEGRAF_VMPASSWORD}"

  tagexclude = ["path", "metric_type"]

# Rename measurements, tags, and fields that pass through this filter.
[[processors.rename]]
  order = 1 
  [[processors.rename.replace]]
    measurement = "tail"
    dest = "${TELEGRAF_FRIDGE}"

# Performs file path manipulations on tags and fields
[[processors.filepath]]
  order = 2

  # Treat the tag value as a path, converting it to its the last element without its suffix
  [[processors.filepath.stem]]
    tag = "path"

# Transforms tag and field values with regex pattern
[[processors.regex]]
  order = 3

  [[processors.regex.tags]]

    key = "path"

    result_key = "channel"

    pattern = "^(CH\\d) (P|R|T) [T\\d\\-]+$"

    replacement = "${1}"

  [[processors.regex.tags]]
    key = "path"

    result_key = "unit"

    pattern = "^(CH\\d) (P|R|T) [T\\d\\-]+$"

    replacement = "${2}"

[[processors.execd]]
  order = 4

  command = ["python", "${TELEGRAF_PYTHON}"]
  
  [processors.execd.tagpass]
    metric_type = ["pressure", "status"]



###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Parse the new lines appended to a file
[[inputs.tail]]
  # files = ["C:\\Users\\QNL\\Documents\\BlizzardLogs\\Logs904\\[0-9][0-9]-[0-9][0-9]-[0-9][0-9]\\CH*"]
  files = ['${TELEGRAF_FRIDGE_LOGDIR}\${TELEGRAF_DATE}\CH*']

  # Read file from beginning.
  from_beginning = ${TELEGRAF_BACKFILL}

  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  watch_method = "poll"

  # Maximum lines of the file to process that have not yet be written by the
  # output.  For best throughput set based on the number of metrics on each
  # line and the size of the output's metric_batch_size.
  max_undelivered_lines = 1500

  data_format = "grok"

  grok_patterns = ['%{BLUEFORS_TIMESTAMP:timestamp:ts-"02-01-06.15:04:05"},%{SCI:temperature:float}']

  grok_custom_patterns = '''
    BLUEFORS_TIMESTAMP (?:\d{2}-\d{2}-\d{2},\d{2}:\d{2}:\d{2})
    SCI -?(?:0|[1-9]\d*)(?:\.\d+)?(?:[eE][+\-]?\d+)?
  '''

  grok_timezone = "Local"

  [inputs.tail.tags]
    metric_type = "temperature"

[[inputs.tail]]
  files = ['${TELEGRAF_FRIDGE_LOGDIR}\${TELEGRAF_DATE}\maxigauge*']

  # Read file from beginning.
  from_beginning = ${TELEGRAF_BACKFILL}

  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  watch_method = "poll"

  # Maximum lines of the file to process that have not yet be written by the
  # output.  For best throughput set based on the number of metrics on each
  # line and the size of the output's metric_batch_size.
  max_undelivered_lines = 1500

  data_format = "grok"

  grok_patterns = ['%{BLUEFORS_TIMESTAMP:timestamp:ts-"02-01-06.15:04:05"},%{GREEDYDATA:pressures:string}']

  grok_custom_patterns = '''
    BLUEFORS_TIMESTAMP (?:\d{2}-\d{2}-\d{2},\d{2}:\d{2}:\d{2})
  '''

  grok_timezone = "Local"

  [inputs.tail.tags]
    metric_type = "pressure"

[[inputs.tail]]
  files = ['${TELEGRAF_FRIDGE_LOGDIR}\${TELEGRAF_DATE}\Flowmeter*']

  # Read file from beginning.
  from_beginning = ${TELEGRAF_BACKFILL}

  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  watch_method = "poll"

  # Maximum lines of the file to process that have not yet be written by the
  # output.  For best throughput set based on the number of metrics on each
  # line and the size of the output's metric_batch_size.
  max_undelivered_lines = 1500

  data_format = "grok"

  grok_patterns = ['%{BLUEFORS_TIMESTAMP:timestamp:ts-"02-01-06.15:04:05"},%{NUMBER:flow:float}']

  grok_custom_patterns = '''
    BLUEFORS_TIMESTAMP (?:\d{2}-\d{2}-\d{2},\d{2}:\d{2}:\d{2})
  '''

  grok_timezone = "Local"

  [inputs.tail.tags]
    metric_type = "flow"

[[inputs.tail]]
  files = ['${TELEGRAF_FRIDGE_LOGDIR}\${TELEGRAF_DATE}\Status*']

  # Read file from beginning.
  from_beginning = ${TELEGRAF_BACKFILL}

  ## Method used to watch for file updates.  Can be either "inotify" or "poll".
  watch_method = "poll"

  # Maximum lines of the file to process that have not yet be written by the
  # output.  For best throughput set based on the number of metrics on each
  # line and the size of the output's metric_batch_size.
  max_undelivered_lines = 1500

  data_format = "grok"

  grok_patterns = ['%{BLUEFORS_TIMESTAMP:timestamp:ts-"02-01-06.15:04:05"},%{GREEDYDATA:status:string}']

  grok_custom_patterns = '''
    BLUEFORS_TIMESTAMP (?:\d{2}-\d{2}-\d{2},\d{2}:\d{2}:\d{2})
  '''

  grok_timezone = "Local"

  [inputs.tail.tags]
    metric_type = "status"